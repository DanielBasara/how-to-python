{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## logits\n",
    "但在深度学习中，logits就是最终的全连接层的输出，而非其本意。通常神经网络中都是先有logits，而后通过sigmoid函数或者softmax函数得到概率 [公式] 的，所以大部分情况下都无需用到logit函数的表达式。\n",
    "\n",
    "## logit函数\n",
    "$$L(p)=ln \\frac{p}{1-p}$$\n",
    "是sigmoid函数（标准logistic函数）的反函数，目的是将取值范围在$[0,1]$内的概率映射到实数域$[-inf,inf]$的函数，如果p=0.5，函数值为0,P<0.5，函数值为负，p>0.5，函数值为正。反过来softmax和sigmoid则是将$[-inf,inf]$映射到$[0,1]$。\n",
    "\n",
    "## sigmoid函数\n",
    "$$p(x)=\\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "## sigmoid_cross_entropy_with_logits()\n",
    "代码 ：\n",
    "```python\n",
    "tf.nn.sigmoid_cross_entropy_with_logits(labels=None, logits=None, name=None)\n",
    "```\n",
    "计算网络输出logits和标签labels的sigmoid cross entropy loss（任务的类与类之间不互斥）\n",
    "\n",
    "## tf.reduce_sum()\n",
    "**axis是多维数组每个维度的坐标**最外层的axis是0，越往里越大。\n",
    "\n",
    "`tf.reduce_sum(a,axis=1)`就是将axis=1(从外往里第二个维度)的数组相加的结果，并去掉一个维度。\n",
    "\n",
    "**axis为负数时表示从内往外倒数第x个维度**\n",
    "\n",
    "reduce表示“归约”如果代码是`tf.reduce_sum(a,axis=1,keepdims=True)`表示不归约，即不去掉axis=1的维度，单纯求和。\n",
    "\n",
    "`tf.reduce_sum(a,[0,1])`表示先对0维求和再对1维求和。\n",
    "\n",
    "## Evidence Lower BOund（ELBO） loss 证据下界损失函数\n",
    "$$\\log p(x)\\geq E_{q_\\phi (z|x)}[\\log p_\\phi (x|z)]-D_{KL}[q_\\phi (z|x)\\parallel p(z)]$$\n",
    "\n",
    "## cross entropy交叉熵\n",
    "可以用来表示从事件A的角度看，如何描述事件B\n",
    "\n",
    "## KL散度(KLダイバージェンス)\n",
    "可以用来表示从事件A的角度来看，事件B有多大不同。KL散度也可以被用于计算代价，而在特定情况下最小化KL散度等价于最小化交叉熵。而交叉熵的运算更简单，所以常用于计算代价。\n",
    "\n",
    "## 元学习meta-learning（有趣的研究方向）\n",
    "为了解决学习如何学习的问题。即通过另一个模型来针对神经网络中的初始参数和优化器参数进行调整，从而实现神经网络较好的学习效率。\n",
    "+ 机器学习与元学习的区别：\n",
    "    + machine learning：通过训练数据，学习到输入$X$与输出$Y$之间的映射，从而找到函数$f$，使得$y=f(x)$\n",
    "    + meta learning:通过很多的训练任务$T$及对应的训练数据$D，找到函数$F$。$F可以输出一个函数$f$，用于新的任务。\n",
    "\n",
    "## t-SNE\n",
    "是一种降维技术，用于在二维或三维的低维空间中表示高维数据集，从而使其可视化。与其他降维算法(如PCA)相比，t-SNE创建了一个缩小的特征空间，相似的样本由附近的点建模，不相似的样本由高概率的远点建模。在高水平上，t-SNE为高维样本构建了一个概率分布，相似的样本被选中的可能性很高，而不同的点被选中的可能性极小。然后，t-SNE为低维嵌入中的点定义了相似的分布。最后，t-SNE最小化了两个分布之间关于嵌入点位置的Kullback-Leibler（KL)散度。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2, 2), dtype=int32, numpy=\n",
       "array([[[2, 2],\n",
       "        [2, 2]]])>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "81c9043bd05d90ad341e64bda8f89d527933a2e8877b3e37e2c924c4817d5fff"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
